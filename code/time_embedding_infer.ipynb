{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "for data_name in ['Dai']:\n",
    "    mu_embeddings = []\n",
    "    sigma_embeddings = []\n",
    "    for month in range(4,8):\n",
    "        file_path = f'../outputs/new_time_embedding/task2/{data_name}/train/{month}/epoch_500_k_1_lr_0.0001_initalembed_no_seed_10/rglossfn_tweedie_activate_softplus_rgweight_1.0_lpweight_0.0_rankweight_0.0_conweight_0.0_diffweight_1.0_gaussian_yes_crossattn_yes_bias_yes_node.pt'\n",
    "        x = torch.load(file_path)\n",
    "        data = pd.read_csv(f'../data/{data_name}/task2/{month}/triplet_percentage.tsv', header=None, sep='\\t')\n",
    "        entities = pd.read_csv(f'../data/{data_name}/task1/entities.dict', sep='\\t', header=None)\n",
    "        user_num = sum(entities[1] < 30000)\n",
    "        item_num = sum(entities[1] >= 30000)\n",
    "        mu_embedding = torch.randn(len(entities), 768).to(x[0].device)\n",
    "        mu_embedding[torch.cat((torch.from_numpy(data[0].unique()) - 20000, torch.from_numpy(data[2].unique()) - 30000 + user_num))] = x[0]\n",
    "        sigma_embedding = torch.randn(len(entities), 768).to(x[0].device)\n",
    "        sigma_embedding[torch.cat((torch.from_numpy(data[0].unique()) - 20000, torch.from_numpy(data[2].unique()) - 30000 + user_num))] = x[1]\n",
    "        mu_embeddings.append(mu_embedding.unsqueeze(0))\n",
    "        sigma_embeddings.append(sigma_embedding.unsqueeze(0))\n",
    "    mu_embeddings = torch.cat(mu_embeddings, dim=0)\n",
    "    sigma_embeddings = torch.cat(sigma_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], loss_reconstruction: 0.005229885224252939, loss_prediction: 0.049811359494924545, loss_attract: 0.005293694790452719, loss_repel: 0.07053935527801514:   0%|          | 0/100 [00:03<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TimeSeriesAutoEncoder(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(TimeSeriesAutoEncoder, self).__init__()\n",
    "        # 稳定项MLP\n",
    "        self.stable_mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "        # 趋势项MLP\n",
    "        self.trend_mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "        # 预测下一时间片的MLP\n",
    "        self.next_step_mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        stable = self.stable_mlp(x)\n",
    "        trend = self.trend_mlp(x)\n",
    "        reconstructed = stable + trend  # 重构原始embedding\n",
    "        next_step = self.next_step_mlp(trend) + stable  # 预测下一个时间片\n",
    "        return reconstructed, next_step, stable, trend\n",
    "\n",
    "# 超参数\n",
    "dim = 768  # 示例embedding维度\n",
    "learning_rate = 0.001\n",
    "epochs = 100  # 示例训练轮数\n",
    "\n",
    "model = TimeSeriesAutoEncoder(dim).to('cuda')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_data = mu_embeddings\n",
    "temperature = 10\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_time_slice_similarity(embeddings):\n",
    "    T, num, dim = embeddings.shape\n",
    "    embeddings_flat = embeddings.reshape(T, -1)  # 将每个时间片的embeddings展平\n",
    "    sim = F.cosine_similarity(embeddings_flat[:, None, :], embeddings_flat[None, :, :], dim=2)\n",
    "    return sim\n",
    "\n",
    "def attraction_loss(embeddings, margin=2.0, temperature=2):\n",
    "    sim = compute_time_slice_similarity(embeddings) / temperature\n",
    "    mask = ~torch.eye(sim.size(0), dtype=torch.bool)  # 排除自相似\n",
    "    return (margin - sim[mask]).mean()\n",
    "\n",
    "def repulsion_loss(embeddings, margin=2.0, temperature=2):\n",
    "    sim = compute_time_slice_similarity(embeddings) / temperature\n",
    "    mask = ~torch.eye(sim.size(0), dtype=torch.bool)  # 排除自距离\n",
    "    return (sim[mask] + margin).mean()\n",
    "\n",
    "# 训练过程\n",
    "from tqdm import tqdm\n",
    "with tqdm(range(epochs)) as bar:\n",
    "    for epoch in range(epochs):\n",
    "        reconstructed, predicted_next_step, stable, trend  = model(train_data)\n",
    "        loss_reconstruction = criterion(reconstructed, train_data)\n",
    "        loss_prediction = criterion(predicted_next_step[:-1], train_data[1:])\n",
    "        loss_attract = attraction_loss(stable, margin=1/temperature, temperature=temperature)\n",
    "        loss_repel = repulsion_loss(trend, margin=1/temperature, temperature=temperature)\n",
    "        loss = loss_reconstruction + loss_prediction + loss_attract + loss_repel\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bar.set_description(f\"Epoch [{epoch+1}/{epochs}], loss_reconstruction: {loss_reconstruction.item()}, loss_prediction: {loss_prediction.item()}, loss_attract: {loss_attract.item()}, loss_repel: {loss_repel.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2001, -0.1920, -0.2176,  ..., -0.0677,  0.0109,  0.1722],\n",
       "        [-0.2562, -0.1960, -0.3001,  ..., -0.1479,  0.0238,  0.2247],\n",
       "        [-0.2086, -0.1850, -0.1926,  ..., -0.0231,  0.0203,  0.1444],\n",
       "        ...,\n",
       "        [-0.3193,  0.0074, -0.2818,  ..., -0.1074,  0.0467,  0.0702],\n",
       "        [-0.2678, -0.0127, -0.2537,  ..., -0.1075,  0.0036,  0.0861],\n",
       "        [-0.2355, -0.0152, -0.2440,  ..., -0.0941,  0.0083,  0.0781]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, test_predicted_next_step, _, _  = model(train_data)\n",
    "test_predicted_next_step[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pyg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b178d1d1f20fb67494d919dfa3876b9c6cf12833c07c4995b1cd48ff68ca01f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
